{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e32033",
   "metadata": {},
   "source": [
    "# ğŸ¯ XGBoost Hyperparameter Tuning\n",
    "\n",
    "**Team:** Error 400  \n",
    "**Dataset:** UK Housing Prices (1995-2017)  \n",
    "**Purpose:** Fine-tune XGBoost model to achieve best possible performance\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ What is Hyperparameter Tuning?\n",
    "\n",
    "Our XGBoost model has many \"settings\" (hyperparameters) that control how it learns:\n",
    "- **n_estimators** - How many trees to build\n",
    "- **max_depth** - How deep each tree can grow\n",
    "- **learning_rate** - How fast the model learns\n",
    "- And many more...\n",
    "\n",
    "**Default settings** gave us RÂ² = 63%. But with the **right settings**, we can do even better!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Our Goal\n",
    "\n",
    "- **Current Performance:** RÂ² = 63.0%, MAE = Â£50,689\n",
    "- **Target:** RÂ² > 65%, MAE < Â£48,000\n",
    "\n",
    "Let's find the best hyperparameters! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e69d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Step 1: Setup Environment\n",
    "\n",
    "Import required libraries and load our clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Import Libraries\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder  # â† MAKE SURE THIS IS HERE!\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0bb3dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading clean data...\n",
      "âœ… Loaded 5,972,822 records\n",
      "ğŸ“… Date range: 1995 - 2017\n",
      "ğŸ’° Price range: Â£6,000 - Â£1,320,509\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>town_city</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>property_type_label</th>\n",
       "      <th>is_new_build</th>\n",
       "      <th>tenure_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18899</td>\n",
       "      <td>WAKEFIELD</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>WEST YORKSHIRE</td>\n",
       "      <td>1995</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Semi-Detached</td>\n",
       "      <td>False</td>\n",
       "      <td>Freehold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82000</td>\n",
       "      <td>GREENFORD</td>\n",
       "      <td>EALING</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>1995</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Semi-Detached</td>\n",
       "      <td>False</td>\n",
       "      <td>Freehold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55000</td>\n",
       "      <td>SWINDON</td>\n",
       "      <td>THAMESDOWN</td>\n",
       "      <td>THAMESDOWN</td>\n",
       "      <td>1995</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Semi-Detached</td>\n",
       "      <td>False</td>\n",
       "      <td>Freehold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28500</td>\n",
       "      <td>BOOTLE</td>\n",
       "      <td>SEFTON</td>\n",
       "      <td>MERSEYSIDE</td>\n",
       "      <td>1995</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>False</td>\n",
       "      <td>Freehold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76000</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>WEST YORKSHIRE</td>\n",
       "      <td>1995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Detached</td>\n",
       "      <td>False</td>\n",
       "      <td>Freehold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  town_city    district          county  year  month  quarter  \\\n",
       "0  18899  WAKEFIELD       LEEDS  WEST YORKSHIRE  1995      6        2   \n",
       "1  82000  GREENFORD      EALING  GREATER LONDON  1995      6        2   \n",
       "2  55000    SWINDON  THAMESDOWN      THAMESDOWN  1995      3        1   \n",
       "3  28500     BOOTLE      SEFTON      MERSEYSIDE  1995     10        4   \n",
       "4  76000      LEEDS       LEEDS  WEST YORKSHIRE  1995      1        1   \n",
       "\n",
       "   day_of_week property_type_label  is_new_build tenure_label  \n",
       "0            4       Semi-Detached         False     Freehold  \n",
       "1            4       Semi-Detached         False     Freehold  \n",
       "2            4       Semi-Detached         False     Freehold  \n",
       "3            3            Terraced         False     Freehold  \n",
       "4            2            Detached         False     Freehold  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyCaret\n",
    "from pycaret.regression import *\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# ==========================================\n",
    "# Load and Prepare Data\n",
    "# ==========================================\n",
    "\n",
    "print(\"ğŸ“‚ Loading clean data...\")\n",
    "df = pd.read_parquet(\"data/cleaned/housing_FULL_clean.parquet\")\n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} records\")\n",
    "print(f\"ğŸ“… Date range: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"ğŸ’° Price range: Â£{df['price'].min():,} - Â£{df['price'].max():,}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ff6cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Step 2: Prepare Features\n",
    "\n",
    "Apply the same feature engineering we used in PyCaret:\n",
    "1. Create smart location encoding (top towns/districts)\n",
    "2. Select relevant features\n",
    "3. Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d76a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Creating engineered features...\n",
      "\n",
      "âœ“ Towns: 1167 â†’ 31 categories\n",
      "âœ“ Districts: 453 â†’ 51 categories\n",
      "\n",
      "âœ… Features prepared: (5972822, 9)\n",
      "ğŸ¯ Target: (5972822,)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Feature Engineering\n",
    "# ==========================================\n",
    "\n",
    "print(\"ğŸ”§ Creating engineered features...\\n\")\n",
    "\n",
    "# 1. Smart location encoding\n",
    "top_towns = df['town_city'].value_counts().head(30).index\n",
    "df['town_encoded'] = df['town_city'].apply(lambda x: x if x in top_towns else 'OTHER')\n",
    "\n",
    "top_districts = df['district'].value_counts().head(50).index\n",
    "df['district_encoded'] = df['district'].apply(lambda x: x if x in top_districts else 'OTHER')\n",
    "\n",
    "print(f\"âœ“ Towns: {df['town_city'].nunique()} â†’ {df['town_encoded'].nunique()} categories\")\n",
    "print(f\"âœ“ Districts: {df['district'].nunique()} â†’ {df['district_encoded'].nunique()} categories\")\n",
    "\n",
    "# 2. Select features\n",
    "feature_columns = [\n",
    "    'property_type_label',\n",
    "    'is_new_build',\n",
    "    'tenure_label',\n",
    "    'county',\n",
    "    'district_encoded',\n",
    "    'town_encoded',\n",
    "    'year',\n",
    "    'month',\n",
    "    'quarter'\n",
    "]\n",
    "\n",
    "X = df[feature_columns].copy()\n",
    "y = df['price'].copy()\n",
    "\n",
    "print(f\"\\nâœ… Features prepared: {X.shape}\")\n",
    "print(f\"ğŸ¯ Target: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e26a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¢ Step 3: Encode Categorical Variables & Split Data\n",
    "\n",
    "Convert text categories to numbers and split into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a219120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Encoding categorical variables...\n",
      "\n",
      "  âœ“ Encoded property_type_label: 5 categories\n",
      "  âœ“ Encoded tenure_label: 3 categories\n",
      "  âœ“ Encoded county: 127 categories\n",
      "  âœ“ Encoded district_encoded: 51 categories\n",
      "  âœ“ Encoded town_encoded: 31 categories\n",
      "\n",
      "âœ… Encoding complete! Shape: (5972822, 9)\n",
      "\n",
      "âœ‚ï¸ Splitting data...\n",
      "âœ… Training set: 4,778,257 records\n",
      "âœ… Test set: 1,194,565 records\n",
      "\n",
      "ğŸ“Š Ready for tuning!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Encode Categorical Variables\n",
    "# ==========================================\n",
    "\n",
    "print(\"ğŸ”¢ Encoding categorical variables...\\n\")\n",
    "\n",
    "X_encoded = X.copy()\n",
    "\n",
    "# Convert boolean to int\n",
    "X_encoded['is_new_build'] = X_encoded['is_new_build'].astype(int)\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_cols = ['property_type_label', 'tenure_label', 'county', 'district_encoded', 'town_encoded']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"  âœ“ Encoded {col}: {len(le.classes_)} categories\")\n",
    "\n",
    "print(f\"\\nâœ… Encoding complete! Shape: {X_encoded.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# Train/Test Split\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nâœ‚ï¸ Splitting data...\")\n",
    "\n",
    "# To this:\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,  # âœ… Use original X with strings\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Training set: {len(X_train):,} records\")\n",
    "print(f\"âœ… Test set: {len(X_test):,} records\")\n",
    "print(f\"\\nğŸ“Š Ready for tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c108ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ² Training baseline XGBoost with default parameters...\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š BASELINE XGBOOST PERFORMANCE (Default Parameters)\n",
      "============================================================\n",
      "RÂ² Score:                    0.5860 (58.60%)\n",
      "Mean Absolute Error (MAE):   Â£52,424\n",
      "Root Mean Squared Error:     Â£91,221\n",
      "============================================================\n",
      "\n",
      "ğŸ’¡ This is our starting point. Let's improve it with tuning!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Baseline XGBoost Model\n",
    "# ==========================================\n",
    "\n",
    "print(\"ğŸŒ² Training baseline XGBoost with default parameters...\\n\")\n",
    "\n",
    "# Train with defaults\n",
    "baseline_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "baseline_mae = mean_absolute_error(y_test, y_pred_baseline)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š BASELINE XGBOOST PERFORMANCE (Default Parameters)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RÂ² Score:                    {baseline_r2:.4f} ({baseline_r2*100:.2f}%)\")\n",
    "print(f\"Mean Absolute Error (MAE):   Â£{baseline_mae:,.0f}\")\n",
    "print(f\"Root Mean Squared Error:     Â£{baseline_rmse:,.0f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ’¡ This is our starting point. Let's improve it with tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f9f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records loaded: 5,972,822\n",
      "Price stats:\n",
      "count    5.972822e+06\n",
      "mean     1.666644e+05\n",
      "std      1.424276e+05\n",
      "min      6.000000e+03\n",
      "25%      7.500000e+04\n",
      "50%      1.300000e+05\n",
      "75%      2.100000e+05\n",
      "max      1.320509e+06\n",
      "Name: price, dtype: float64\n",
      "\n",
      "Price range: Â£6,000 - Â£1,320,509\n"
     ]
    }
   ],
   "source": [
    "print(f\"Records loaded: {len(df):,}\")\n",
    "print(f\"Price stats:\")\n",
    "print(df['price'].describe())\n",
    "print(f\"\\nPrice range: Â£{df['price'].min():,} - Â£{df['price'].max():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e5ab4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Step 5: Hyperparameter Tuning\n",
    "\n",
    "We'll use **RandomizedSearchCV** to test different combinations of hyperparameters:\n",
    "\n",
    "**Key Parameters to Tune:**\n",
    "- **n_estimators** - Number of trees (more = better but slower)\n",
    "- **max_depth** - Tree depth (deeper = more complex)\n",
    "- **learning_rate** - How fast model learns (lower = slower but better)\n",
    "- **subsample** - Fraction of data used per tree\n",
    "- **colsample_bytree** - Fraction of features used per tree\n",
    "- **min_child_weight** - Minimum data in leaf nodes\n",
    "\n",
    "**Method:** Test 50 random combinations, keep the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b599212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Setting up hyperparameter search...\n",
      "\n",
      "âš ï¸  With 20M records, we'll use a smart strategy:\n",
      "   1. Tune on 1M sample (fast)\n",
      "   2. Apply best params to full dataset\n",
      "\n",
      "ğŸ“Š Creating 1,000,000 record sample for tuning...\n",
      "âœ… Sample created: 1,000,000 records\n",
      "\n",
      "ğŸ“‹ Optimized parameters to tune:\n",
      "  â€¢ n_estimators: [300, 500, 800, 1000]\n",
      "  â€¢ max_depth: [5, 7, 9, 12, 15]\n",
      "  â€¢ learning_rate: [0.01, 0.03, 0.05, 0.08, 0.1]\n",
      "  â€¢ subsample: [0.7, 0.8, 0.9]\n",
      "  â€¢ colsample_bytree: [0.7, 0.8, 0.9]\n",
      "  â€¢ min_child_weight: [1, 3, 5]\n",
      "  â€¢ gamma: [0, 0.1, 0.2]\n",
      "  â€¢ reg_alpha: [0, 0.1, 0.5]\n",
      "  â€¢ reg_lambda: [1, 1.5, 2]\n",
      "\n",
      "â³ Starting random search on 1M sample...\n",
      "Testing 20 combinations with 2-fold CV (faster but still reliable)...\n",
      "\n",
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
      "\n",
      "âœ… Tuning complete on sample!\n",
      "\n",
      "ğŸš€ Now training best model on FULL dataset...\n",
      "\n",
      "ğŸ† Best parameters: {'subsample': 0.8, 'reg_lambda': 1.5, 'reg_alpha': 0.1, 'n_estimators': 800, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.05, 'gamma': 0.2, 'colsample_bytree': 0.8}\n",
      "\n",
      "âœ… Final model trained on full dataset!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Hyperparameter Tuning - Smart Approach for Large Dataset\n",
    "# ==========================================\n",
    "\n",
    "print(\"ğŸ” Setting up hyperparameter search...\\n\")\n",
    "print(\"âš ï¸  With 20M records, we'll use a smart strategy:\")\n",
    "print(\"   1. Tune on 1M sample (fast)\")\n",
    "print(\"   2. Apply best params to full dataset\\n\")\n",
    "\n",
    "# Create 1M sample for tuning\n",
    "sample_size = 1_000_000\n",
    "print(f\"ğŸ“Š Creating {sample_size:,} record sample for tuning...\")\n",
    "\n",
    "sample_indices = np.random.choice(X_train.index, size=min(sample_size, len(X_train)), replace=False)\n",
    "X_train_sample = X_train.loc[sample_indices]\n",
    "y_train_sample = y_train.loc[sample_indices]\n",
    "\n",
    "print(f\"âœ… Sample created: {len(X_train_sample):,} records\\n\")\n",
    "\n",
    "# Improved parameter grid (based on research for housing prices)\n",
    "param_distributions = {\n",
    "    'n_estimators': [300, 500, 800, 1000],           # More trees\n",
    "    'max_depth': [5, 7, 9, 12, 15],                  # Deeper trees\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1], # Lower learning rates\n",
    "    'subsample': [0.7, 0.8, 0.9],                    # Sample ratio\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],            # Feature ratio\n",
    "    'min_child_weight': [1, 3, 5],                   # Regularization\n",
    "    'gamma': [0, 0.1, 0.2],                          # Pruning\n",
    "    'reg_alpha': [0, 0.1, 0.5],                      # L1 regularization\n",
    "    'reg_lambda': [1, 1.5, 2]                        # L2 regularization\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Optimized parameters to tune:\")\n",
    "for param, values in param_distributions.items():\n",
    "    print(f\"  â€¢ {param}: {values}\")\n",
    "\n",
    "# Create base model\n",
    "xgb_model = XGBRegressor(random_state=42, n_jobs=-1, tree_method='hist')  # 'hist' = faster!\n",
    "\n",
    "# RandomizedSearchCV on SAMPLE\n",
    "print(\"\\nâ³ Starting random search on 1M sample...\")\n",
    "print(\"Testing 20 combinations with 2-fold CV (faster but still reliable)...\\n\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,              # 20 combinations (good balance)\n",
    "    scoring='r2',\n",
    "    cv=2,                   # 2-fold (faster)\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on SAMPLE\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(\"\\nâœ… Tuning complete on sample!\")\n",
    "print(\"\\nğŸš€ Now training best model on FULL dataset...\")\n",
    "\n",
    "# Get best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"\\nğŸ† Best parameters: {best_params}\")\n",
    "\n",
    "# Train final model on FULL dataset with best parameters\n",
    "final_model = XGBRegressor(**best_params, random_state=42, n_jobs=-1, tree_method='hist')\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nâœ… Final model trained on full dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9460f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ† Step 6: Best Parameters & Performance\n",
    "\n",
    "Let's see what parameters performed best and how much we improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ed9c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† BEST PARAMETERS FOUND:\n",
      "============================================================\n",
      "  subsample            = 0.8\n",
      "  reg_lambda           = 1.5\n",
      "  reg_alpha            = 0.1\n",
      "  n_estimators         = 800\n",
      "  min_child_weight     = 1\n",
      "  max_depth            = 7\n",
      "  learning_rate        = 0.05\n",
      "  gamma                = 0.2\n",
      "  colsample_bytree     = 0.8\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š CROSS-VALIDATION RESULTS:\n",
      "============================================================\n",
      "Best CV RÂ² Score: 0.5860 (58.60%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TEST SET PERFORMANCE (Tuned Model):\n",
      "============================================================\n",
      "RÂ² Score:                    0.5891 (58.91%)\n",
      "Mean Absolute Error (MAE):   Â£51,971\n",
      "Root Mean Squared Error:     Â£90,888\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ IMPROVEMENT OVER BASELINE:\n",
      "============================================================\n",
      "RÂ² Improvement:     58.60% â†’ 58.91% (+0.30%)\n",
      "MAE Improvement:    Â£52,424 â†’ Â£51,971 (-Â£453)\n",
      "RMSE Improvement:   Â£91,221 â†’ Â£90,888 (-Â£334)\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ TARGET ACHIEVEMENT:\n",
      "RÂ² Target (70%):      âŒ Not yet (58.9%)\n",
      "MAE Target (<Â£45k):   âŒ Not yet (Â£51,971)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Display Best Parameters and Results\n",
    "# ==========================================\n",
    "\n",
    "print(\"ğŸ† BEST PARAMETERS FOUND:\")\n",
    "print(\"=\"*60)\n",
    "best_params = random_search.best_params_\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param:<20} = {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š CROSS-VALIDATION RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best CV RÂ² Score: {random_search.best_score_:.4f} ({random_search.best_score_*100:.2f}%)\")\n",
    "\n",
    "# Get best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Test on holdout set\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
    "tuned_mae = mean_absolute_error(y_test, y_pred_tuned)\n",
    "tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š TEST SET PERFORMANCE (Tuned Model):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RÂ² Score:                    {tuned_r2:.4f} ({tuned_r2*100:.2f}%)\")\n",
    "print(f\"Mean Absolute Error (MAE):   Â£{tuned_mae:,.0f}\")\n",
    "print(f\"Root Mean Squared Error:     Â£{tuned_rmse:,.0f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RÂ² Improvement:     {baseline_r2:.2%} â†’ {tuned_r2:.2%} (+{(tuned_r2-baseline_r2)*100:.2f}%)\")\n",
    "print(f\"MAE Improvement:    Â£{baseline_mae:,.0f} â†’ Â£{tuned_mae:,.0f} (-Â£{baseline_mae-tuned_mae:,.0f})\")\n",
    "print(f\"RMSE Improvement:   Â£{baseline_rmse:,.0f} â†’ Â£{tuned_rmse:,.0f} (-Â£{baseline_rmse-tuned_rmse:,.0f})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Did we reach our goal?\n",
    "print(\"\\nğŸ¯ TARGET ACHIEVEMENT:\")\n",
    "target_r2 = 0.70\n",
    "target_mae = 45000\n",
    "print(f\"RÂ² Target (70%):      {'âœ… ACHIEVED!' if tuned_r2 >= target_r2 else f'âŒ Not yet ({tuned_r2*100:.1f}%)'}\")\n",
    "print(f\"MAE Target (<Â£45k):   {'âœ… ACHIEVED!' if tuned_mae <= target_mae else f'âŒ Not yet (Â£{tuned_mae:,.0f})'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d21145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
